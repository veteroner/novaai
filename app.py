from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os

# Teknova Nova AI - Ã–zgÃ¼n Model
app = FastAPI(
    title="Teknova Nova AI", 
    description="Teknova'nÄ±n Ã¶zgÃ¼n Nova AI modeli - Token gerektirmez",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["Access-Control-Allow-Origin"]
)

# Nova AI Model yÃ¼klemesi - Ã–zgÃ¼n teknoloji
model_path = "./nova-ai-model"  # Kendi Nova AI modeliniz
colab_path = "/content/nova-ai-model"  # Colab iÃ§in path

# Path kontrolÃ¼
actual_path = colab_path if os.path.exists(colab_path) else model_path

print("ğŸš€ Teknova Nova AI modeli yÃ¼kleniyor... (Web API)")
print("ğŸŒŸ Bu tamamen Ã¶zgÃ¼n bir Teknova Nova AI modelidir!")
print("ğŸ’¡ Hugging Face token gerektirmez - kendi modeliniz!")

try:
    tokenizer = AutoTokenizer.from_pretrained(actual_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        actual_path, 
        torch_dtype=torch.float16, 
        device_map="auto",
        trust_remote_code=True
    )
    print("âœ… Teknova Nova AI Web API hazÄ±r!")
    print("ğŸ‰ Ã–zgÃ¼n Nova AI teknolojisi aktif!")
except Exception as e:
    print(f"âŒ Nova AI model yÃ¼kleme hatasÄ±: {e}")
    print("ğŸ’¡ Nova AI model dosyalarÄ±nÄ±zÄ± doÄŸru konuma yÃ¼klediÄŸinizden emin olun.")

@app.get("/")
async def home():
    return HTMLResponse("""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Teknova Nova AI</title>
        <meta charset="utf-8">
        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; 
                   background: linear-gradient(135deg, #ff6b6b, #4ecdc4); 
                   color: white; text-align: center; padding: 50px; }
            .container { background: rgba(255,255,255,0.1); padding: 40px; border-radius: 20px; 
                        backdrop-filter: blur(10px); max-width: 600px; margin: 0 auto; }
            h1 { font-size: 3rem; margin-bottom: 20px; }
            p { font-size: 1.2rem; margin-bottom: 15px; }
            .feature { background: rgba(255,255,255,0.2); padding: 15px; margin: 10px 0; 
                      border-radius: 10px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸš€ Teknova Nova AI</h1>
            <p><strong>Ã–zgÃ¼n yapay zeka teknolojisi</strong></p>
            <div class="feature">
                ğŸŒŸ Tamamen Ã¶zgÃ¼n Teknova Nova AI modeli
            </div>
            <div class="feature">
                âš¡ Token gerektirmez - Kendi modeliniz
            </div>
            <div class="feature">
                ğŸ§  GeliÅŸmiÅŸ AI teknolojisi
            </div>
            <div class="feature">
                ğŸš€ API Endpoint: <strong>/chat</strong>
            </div>
            <p style="margin-top: 30px; font-size: 0.9rem; opacity: 0.8;">
                API kullanÄ±mÄ±: POST /chat {"prompt": "MesajÄ±nÄ±z"}
            </p>
        </div>
    </body>
    </html>
    """)

@app.post("/chat")
async def chat(request: Request):
    try:
        data = await request.json()
        prompt = data.get("prompt", "")
        
        if not prompt.strip():
            return JSONResponse({
                "error": "LÃ¼tfen Nova AI'ya mesajÄ±nÄ±zÄ± yazÄ±n",
                "model": "Teknova Nova AI"
            })
        
        # Nova AI ile konuÅŸma formatÄ±
        conversation = f"KullanÄ±cÄ±: {prompt}\nNova AI:"
        
        inputs = tokenizer(conversation, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        nova_response = response[len(conversation):].strip()
        
        return JSONResponse({
            "response": nova_response,
            "model": "Teknova Nova AI - Ã–zgÃ¼n Model",
            "status": "success"
        }, headers={"Access-Control-Allow-Origin": "*"})
        
    except Exception as e:
        return JSONResponse({
            "error": f"Nova AI hatasÄ±: {str(e)}",
            "model": "Teknova Nova AI",
            "status": "error"
        }, headers={"Access-Control-Allow-Origin": "*"})

if __name__ == "__main__":
    import uvicorn
    print("ğŸŒ Teknova Nova AI Web sunucusu baÅŸlatÄ±lÄ±yor...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
