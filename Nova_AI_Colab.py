# ğŸš€ Teknova Nova AI - Google Colab Ã–zel Scripti
# ğŸŒŸ Tamamen Ã¶zgÃ¼n Teknova Nova AI modeli
# ğŸ’¡ Colab'da kendi modelinizi Ã§alÄ±ÅŸtÄ±rÄ±n

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import zipfile
from google.colab import files

print("ğŸš€ Teknova Nova AI - Google Colab Edition")
print("ğŸŒŸ Tamamen Ã¶zgÃ¼n yapay zeka teknolojisi")
print("=" * 60)

# Colab iÃ§in Nova AI model yolu
MODEL_PATH = "/content/nova-ai-model"

def upload_and_extract_model():
    """Nova AI modelini Colab'a yÃ¼kle ve Ã§Ä±kart"""
    print("ğŸ“‚ Nova AI model dosyalarÄ±nÄ±zÄ± yÃ¼kleyin...")
    print("ğŸ’¡ ZIP dosyasÄ± olarak yÃ¼klemeniz Ã¶nerilir")
    
    # Dosya yÃ¼kleme
    uploaded = files.upload()
    
    for filename in uploaded.keys():
        print(f"ğŸ“¦ Ä°ÅŸleniyor: {filename}")
        
        if filename.endswith('.zip'):
            # ZIP dosyasÄ±nÄ± Ã§Ä±kart
            with zipfile.ZipFile(filename, 'r') as zip_ref:
                zip_ref.extractall(MODEL_PATH)
            print(f"âœ… {filename} baÅŸarÄ±yla Ã§Ä±kartÄ±ldÄ±")
            os.remove(filename)  # ZIP dosyasÄ±nÄ± sil
        else:
            # Tek dosyayÄ± model klasÃ¶rÃ¼ne taÅŸÄ±
            os.makedirs(MODEL_PATH, exist_ok=True)
            os.rename(filename, os.path.join(MODEL_PATH, filename))
            print(f"âœ… {filename} model klasÃ¶rÃ¼ne taÅŸÄ±ndÄ±")
    
    print("ğŸ‰ Nova AI model dosyalarÄ± hazÄ±r!")

def load_nova_ai():
    """Nova AI modelini yÃ¼kle"""
    print("ğŸš€ Teknova Nova AI modeli yÃ¼kleniyor...")
    
    if not os.path.exists(MODEL_PATH):
        print("âŒ Nova AI model klasÃ¶rÃ¼ bulunamadÄ±!")
        print("ğŸ“¤ Ã–nce modelinizi yÃ¼kleyin...")
        upload_and_extract_model()
    
    try:
        # Nova AI Tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, 
            trust_remote_code=True
        )
        print("âœ… Nova AI Tokenizer yÃ¼klendi")
        
        # Nova AI Model - Colab GPU optimizasyonu
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            load_in_8bit=True  # Colab T4 iÃ§in optimize
        )
        print("âœ… Nova AI Model yÃ¼klendi")
        print("ğŸ‰ Teknova Nova AI hazÄ±r!")
        
        return model, tokenizer
    
    except Exception as e:
        print(f"âŒ Nova AI yÃ¼kleme hatasÄ±: {e}")
        return None, None

def nova_chat(message, history, model, tokenizer):
    """Nova AI ile sohbet"""
    if model is None or tokenizer is None:
        return "âŒ Nova AI modeli yÃ¼klenmedi. LÃ¼tfen modeli yÃ¼kleyin."
    
    try:
        # Nova AI konuÅŸma formatÄ±
        conversation = ""
        for user_msg, bot_msg in history:
            conversation += f"KullanÄ±cÄ±: {user_msg}\nNova AI: {bot_msg}\n"
        
        conversation += f"KullanÄ±cÄ±: {message}\nNova AI:"
        
        # Nova AI yanÄ±t Ã¼ret
        inputs = tokenizer(
            conversation, 
            return_tensors="pt", 
            truncation=True, 
            max_length=2048
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        nova_response = response[len(conversation):].strip()
        
        return nova_response
    
    except Exception as e:
        return f"âŒ Nova AI hatasÄ±: {str(e)}"

def create_nova_interface():
    """Nova AI Gradio arayÃ¼zÃ¼ oluÅŸtur"""
    
    # Nova AI modelini yÃ¼kle
    model, tokenizer = load_nova_ai()
    
    # Gradio arayÃ¼zÃ¼
    with gr.Blocks(
        theme=gr.themes.Soft(),
        title="Teknova Nova AI - Colab Edition"
    ) as demo:
        
        gr.HTML("""
        <div style="text-align: center; padding: 20px; background: linear-gradient(135deg, #ff6b6b, #4ecdc4); border-radius: 15px; margin-bottom: 20px;">
            <h1 style="color: white; font-size: 2.5rem; margin: 0;">
                ğŸš€ Teknova Nova AI
            </h1>
            <p style="color: white; font-size: 1.2rem; margin: 10px 0;">
                <strong>Google Colab Edition</strong> - Ã–zgÃ¼n Yapay Zeka
            </p>
            <div style="background: rgba(255,255,255,0.3); padding: 10px; border-radius: 10px; display: inline-block;">
                ğŸŒŸ Tamamen Ã¶zgÃ¼n model â€¢ âš¡ Token gerektirmez â€¢ ğŸ§  Colab GPU optimized
            </div>
        </div>
        """)
        
        chatbot = gr.Chatbot(
            height=500,
            show_label=False,
            show_copy_button=True,
            avatar_images=[None, "ğŸ¤–"]
        )
        
        with gr.Row():
            msg = gr.Textbox(
                placeholder="Nova AI'ya mesajÄ±nÄ±zÄ± yazÄ±n...",
                show_label=False,
                scale=4
            )
            send = gr.Button("ğŸš€ GÃ¶nder", scale=1, variant="primary")
        
        with gr.Row():
            clear = gr.Button("ğŸ—‘ï¸ Temizle")
            reload = gr.Button("ğŸ”„ Model Yenile")
        
        gr.HTML("""
        <div style="text-align: center; padding: 15px; background: #f8f9fa; border-radius: 10px; margin-top: 15px;">
            <h3>ğŸ’¡ Nova AI Colab Rehberi</h3>
            <p>ğŸ”¸ Kendi Nova AI modelinizi ZIP olarak yÃ¼kleyin</p>
            <p>ğŸ”¸ Model otomatik olarak /content/nova-ai-model klasÃ¶rÃ¼ne Ã§Ä±kartÄ±lÄ±r</p>
            <p>ğŸ”¸ T4 GPU ile optimize edilmiÅŸ performans</p>
            <p style="color: #ff6b6b; font-weight: bold;">ğŸš€ Teknova Nova AI - Tamamen Ã–zgÃ¼n</p>
        </div>
        """)
        
        # Event handlers
        def user_message(message, history):
            return "", history + [[message, None]]
        
        def bot_message(history):
            user_message = history[-1][0]
            bot_response = nova_chat(user_message, history[:-1], model, tokenizer)
            history[-1][1] = bot_response
            return history
        
        def reload_model():
            nonlocal model, tokenizer
            model, tokenizer = load_nova_ai()
            return "ğŸ”„ Nova AI modeli yeniden yÃ¼klendi!"
        
        msg.submit(user_message, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot_message, chatbot, chatbot
        )
        send.click(user_message, [msg, chatbot], [msg, chatbot], queue=False).then(
            bot_message, chatbot, chatbot
        )
        clear.click(lambda: None, None, chatbot, queue=False)
        reload.click(reload_model, None, None)
    
    return demo

# Ana fonksiyon
if __name__ == "__main__":
    print("ğŸ¨ Nova AI Colab arayÃ¼zÃ¼ oluÅŸturuluyor...")
    
    demo = create_nova_interface()
    
    print("ğŸŒŸ Nova AI Colab baÅŸlatÄ±lÄ±yor...")
    demo.launch(
        share=True,  # Public link oluÅŸtur
        debug=True,
        show_error=True,
        server_name="0.0.0.0",
        server_port=7860
    ) 